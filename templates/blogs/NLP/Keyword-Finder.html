<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Haneul Kim's - Keyword Finder</title>
	<link rel="stylesheet" href="../../../bootstrap-4.1.3-dist/css/bootstrap.min.css">
	<!-- <link rel="stylesheet" href="css/style.css"> -->
    <link rel="stylesheet" href="../../../static/css/fixed.css">
    <link rel="stylesheet" href="../../../static/css/blogs.css">
	<!-- Font Awesome libarary -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

</head>

<body data-spy="scroll" data-target="#navbarResponsive">


<!-- Navbar -->
<nav class="navbar navbar-expand-sm bg-dark navbar-dark fixed-top">
	<a class="navbar-brand" href="../../../index.html#my-brain">
		take me back.
    </a>
</nav>
<div class="projects">

<div class="row project">
<div class="col-md-3 side-bar">
    <!-- !!!!!!!!!!!!!!!! Later on switch it to flask so I don't have to correct all blogs. -->
    <ul>
        <li><a href="amazon_review.html">Amazon review</a></li>
        <li><b>Web scraping/nlp</b></li>
    </ul>
</div>

<!-- ------------------------------ All Content goes here -------------------- -->
<div class="col-md-6">
    <h2>KeyWord Finder: Data scraping and natural language processing to find keywords in job posts</h1>
    <p class="date">date posted: 2019-05-15</p>
    <br>
    <p>I am not a guy who inputs time into something that I do not find interst in, one of them is reading through bunch of 
        job descriptions and trying to find out what employers are looking for. This project was inspired
        to simply serve my laziness. Hope it helps other lazy people like me. <b>Main goal</b> is
        going through most recent job postings and find most frequently occuring keywords. <span style="color:rgb(228, 31, 31);">
        Not all</span> keywords however only keywords that are technical skills such as: Python, data, C++, etc.
    </p>
    <p>
        Due to lengthy information I will break it down into data scraping section and text classification section
        so feel free to take information you only want.
    </p>
    <p style="font-size:1.5rem;"><u> Data scraping step: </u></p>
    <p>Data will be scraped from glassdoor. Tried to work with Linkedin however
        they do not allow individuals to scrape their data so if you want to 
        work with linkedin data, be prepared to get sued.
    </p>

    <p>As always we import all our dependencies.</p>
    <pre>
    from splinter import Browser
    from bs4 import BeautifulSoup
    import nltk
    import matplotlib.pyplot as plt
    import os
    import pymongo
    import pandas as pd
    import re
    import seaborn as sns
    import time
    </pre>

    <p>Then set up path for chromedriver. It can be downloaded from: <a href="http://chromedriver.chromium.org/downloads">http://chromedriver.chromium.org/downloads</a></p>
    <p>Check the version of chromedriver before downloading as each version serves
        specific version of chrome.
    </p>

    <pre>
    executable_path = {'executable_path': "chromedriver.exe"}
    browser = Browser('chrome', **executable_path, headless=False)
    url = "https://www.glassdoor.ca/index.htm"
    browser.visit(url)
    </pre>
    <p>Notice I gave reference to variable named browser thus browser is used
        to perform activities.
    </p>
    <p>First, visit the glassdoor website then ask the user what job they are 
        looking for and store it into a variable. Second find search bar for 
    job title and insert variable(user input) we just stored. This is same as 
    human being filling the search bar.
    </p>
    <p>
        <b>In this example inputs will be: job = "Data Scientist", location = "Singapore"</b>
    </p>
    <pre>
    browser.visit(url)
    print("What job are you looking for?")
    job = input()
    job_type = browser.find_by_id("KeywordSearch")
    job_type.fill(job)
    </pre>
    <p>If you are not familiar with <a href="https://www.w3schools.com/html/">html</a>, everything on a webpage is stored
        in different, appropriate html tag. In this case job title search bar
        is html input tag with id named KeywordSearch. You can verify by
        pressing f12 on the website, click elements column and press mouse 
        logo then drag your mouse to search bar. This will allow you to view
        all html tags that you hover over.
    </p>
    <p>
        Similarily, do the same for location search. Find id of search bar then
        click it.
    </p>
    <pre>
    print("Where do you want to find your job?")
    job_location = input()
    location = browser.find_by_id("LocationSearch")
    location.fill(job_location)

    browser.find_by_id("HeroSearchButton").click()
    </pre>

    <p>Create a function we will use to scrape each page. It is not getting called 
        yet, however will be called within a for loop.
    </p>

    <pre>
    def scrape():
       <code class="comments"># Getting html of the page then find all lists with class jl.</code> 
       <code class="comments"># jobs variable now contain all job listing's html tags in list form.</code>
        html = browser.html
        soup = BeautifulSoup(html, "html.parser")
        jobs = soup.find_all("li", class_="jl")
        
        <code class="comments"># Loop through each job listing</code>
        for job in jobs:
            <code class="comments"># For now we are scraping preview</code>
            <code class="comments"># Store all info into a appropriate storage.</code>        
            position.append(job.find("div", class_="jobTitle").a.text)
            
            <code class="comments"># name of company and location come in a form like ex: Tommy - Singapore</code>   
            <code class="comments"># so we split along "-", strip white spaces and append to storage</code>
            comp_loc = job.find("div", class_="empLoc").div.text
            comp, loc = comp_loc.split("â€“")
            company.append(comp.strip())
            location.append(loc.strip())
            <code class="comments"># click job listing to see full version.</code>
            browser.click_link_by_href(job.find("a", class_="jobLink")["href"])
            
            <code class="comments"># from current html since if you click job_posting it render new html</code>
            html = browser.html
            soup = BeautifulSoup(html, "html.parser")
            job_desc.append(soup.find("div", class_="desc").text)
            <code class="comments"># Since chromedriver clicks each job listing so quickly sometimes it will miss it therefore
                give it some time.
            </code>         
            time.sleep(3)
    </pre>
    <p>From the bottom of first page find all list that allow us to move through pages</p>
    <pre>
    html = browser.html
    soup = BeautifulSoup(html, "html.parser")
    result = soup.find("div", class_="pagingControls").ul
    <code class="comments"># Note that find_all grab all li tags and store each into list as one item.</code>
    pages = result.find_all("li")
    </pre>
    <p>Create storage to store data, storage we append to in scrape function</p>
    <pre>
    position = []
    exp_level = []
    company = []
    employment_type = []
    location = []
    job_desc = []
    </pre>

    <p>Call scrape function within a for-loop that loops over first 5 pages.</p>
    <pre>
    for page in pages:
    scrape()

    <code class="comments"># for each li tag, run if "a" tag exists. Clicking current page will give error because current pages do not have "a" tag</code>
    if page.a:
        <code class="comments"># Stop if you reach "Next" since we only want first 5 pages.</code>
        if not page.find("li", class_="Next"):          
            try:
                browser.click_link_by_href(page.a['href'])
            except:
                print("This is the last page")
    </pre>

    <p>We have appended all job description into job_desc list where all the words in job description is one item.
        One problem is some of job descriptions are scraped more than once and some are not scraped.
        If you do page by page, on average it has 2 duplicates and others seem fine.
    </p>

    <p>Remove duplicates</p>
    <pre>
    job_desc = set(job_desc)
    job_desc = list(job_desc)
    </pre>
    <p>Set considers distinct items therefore all duplicates are removed then change it back to list</p>

    <p style="font-size:1.5rem;"><u> Text classification step: </u></p>
    <p>As always our first job is cleaning the data. In data science world there are 
        alot of job description that join similar technical skills such as "sql/database",
        "sql/nosql", "C/C++", and so on. We want them separated so lets split along "/" and
        join it back with a comma which will be separated later.
    </p>
    <pre>
    for job in job_desc:
        ", ".join(job.split('/'))
    job_desc = [", ".join(job.split('/')) for job in job_desc]
    </pre>
    <p>Now using nltk we imported in the beginning lowercase all words because we don't want
        our computer to think SQL and sql mean different things.
    </p>
    <pre>
    tok = [nltk.word_tokenize(job.lower()) for job in job_desc]
    </pre>
    <p>If you are unfamiliar with natural language processing concept I recommend read 
        <a href="amazon_review.html" target="_blank"><b style="color:cadetblue;"><u>THIS</u></b></a> before moving on.
    </p>
    <p>Moving on, use stopwords corpus to filter out stopwords and ignore words less than length 2.</p>
    <pre>
    <code class="comments"># import corpus from nltk.</code>
    from nltk.corpus import stopwords

    stop = stopwords.words('english')
    def stopword_deleter(tokenized_job_desc):
        """ ignore stop words, bullets, etc. And put it into one list """
        final_word_list = []
        for lists in tokenized_job_desc:
            for item in lists:
                if len(item)>2 and (item not in stop):
                    <code class="comments"># Some words have \\ at the end, remove them.</code>           
                    final_word_list.append(item.replace("\\",""))
        return final_word_list
    cleaned_list = stopword_deleter(tok)
    </pre>
    <p>passed our tok, and stored it into a variable</p>
    <p>Each Item in cleaned_list is one word. Finally lets lemmatize each word.</p>
    <pre>
    from nltk.stem import WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()
    lemmatized_list = [lemmatizer.lemmatize(word,pos="v") for word in cleaned_list]
    </pre>
    
    <p><span style="color:brown;">Problem:</span> Since most technical skills will overlap 
    it multiple job description <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> cannot be used as it will down weigh overlapping technical
    skills.</p>

    <p>Despite bottleneck printing out most frequent word using nltk FreqDist give us some insight.
    </p>
    <pre>
    freq = nltk.FreqDist(lemmatized_list)
    most_freq_words = freq.most_common(100)
    most_freq_words
    <img src="../../../static/images/posts/keyword-finder/most-freq-word-1.PNG" alt="">
    <img src="../../../static/images/posts/keyword-finder/most-freq-word-2.PNG" alt="">
    </pre>

    <p>We can see words like "statistical", "data", "python", "computer", and "algorithms". Even though
        it does not give us much, we could still figure out what field this job belongs to.
    </p>
    <p><span style="color:brown;">Problem: </span>In data science there are many words 
    that consist of two chunks such as "machine learning", "big data", "business intelligence", 
and more. In the output we can see "data", "machine", "big" however that does not give us much.</p>

    <p><span style="color:cadetblue">Remedy: </span>Use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> from scikit-learn and 
    give it <a href="https://en.wikipedia.org/wiki/N-gram">n-gram</a> parameters. For simplicity I am going to consider range of (1,2)-grams
    </p>
    <p><b>NOTE:</b> As said earlier if you do not understand what CountVectorizer and n-grams are please
        read my blog on how these are used --> <a href="amazon_review.html">CLICK HERE</a> It is 
        essential to understand the process thus you can manipulate, implement and create other techniques. There are
        multiple different methods and <b>ALL</b> existing models have bottlenecks. Copy and pasting
        other's work will all be automated soon (I think) so you have to be the person who really understand 
        what is going on thus able to improve models.
    </p>

    <pre>
    def get_top_n2_words(corpus, n=None):
        <code class="comments"># Consider 2-grams grabbing top 2000 most occuring term</code>
        vec1 = CountVectorizer(ngram_range=(2,2), max_features=2000).fit(corpus) 
        bag_of_words = vec1.transform(corpus) <code class="comments"># Create sparce matrix where cells include freq of occurence. </code>
        sum_words = bag_of_words.sum(axis=0)
        <code class="comments"># sum_words contain frequency of occurence for each word in a list therefore get word from vocab and give it
            frequency by calling sum_words[0, nth word]</code>
        words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]
        <code class="comments"># Sort by number. since ("job", 93). x[1] = 93. In descending order.</code>
        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
        return words_freq[:n]

    top2_words = get_top_n2_words(lemmatized_list, n=100)
    top2_df = pd.DataFrame(top2_words)
    top2_df.columns=["Bi-gram", "Freq"]
    </pre>
    <p>Then we create dataframe using tuple for easy view.</p>
    <pre>
    top2_df.head(15)
    <img src="../../../static/images/posts/keyword-finder/2-gram-df.PNG" alt="">
    </pre>
    <p>Few important words to consider, "scikit-learn"and "problem solving". Even though
        it doesn't do a good job of extracting technical keywords we can get information about
        general keywords to use on our resume for example: "problem solver", "adapt to fast paced environment" and so on.
    </p>

    <p>This was just to show 2-grams, now increase our range in ngram_range parameter in CountVectorizer to
        ngram_range=(1,2). Let's visualize.
    </p>
    <pre>
        <code class="comments"># only top 30 words.</code>
        <code class="comments"># passing in to function we created above <b style="color: crimson;">NOTE:</b> Changed range of n-gram</code>
        top_words = get_top_n2_words(lemmatized_list, n=30) 
        top_df = pd.DataFrame(top_words)
        top_df.columns=["Word", "Freq"]
        sns.set(rc={'figure.figsize':(13,8)})
        g = sns.barplot(x="Word", y="Freq", data=top_df)
        g.set_title("1,2-gram words")
        g.set_xticklabels(g.get_xticklabels(), rotation=30);
    </pre>
    <img src="../../../static/images/posts/keyword-finder/vis-1.png" alt="">

    <p>Finally there are many corpus such as <a href="https://cso.kmi.open.ac.uk/home">CSO</a> which consists of 14k technical words in computer science field.
    Find a corpus that contain all technical skills of job you are looking for and filter. Just as how we filtered stopwords using 
    stopword corpus add extra condition ex: if len(item)>2 and (item not in stop) and (item in cso): </p>
    
    <hr>
    <hr>
    <p>
        I will work to refine this project in order to perfectly serve my laziness. Thank you and give me 
        a feedback.
    </p>
    <br>
    <p>full code: <a href="https://github.com/HaneulKim214/Keyword-Finder">https://github.com/HaneulKim214/Keyword-Finder</a></p>
</div>

<!-- ---------------------------- All Content End here -------------------------- -->
<div class="col-md-3">
</div>

</div>

</div>
</body>

</html>